authors:
- Yen-Chun Chen
- Linjie Li
- Licheng Yu
- Ahmed El Kholy
- Faisal Ahmed
- Zhe Gan
- Yu Cheng
- Jingjing Liu
badges:
- id: OPEN_ACCESS
corpusId: 202889174
fieldsOfStudy:
- Computer Science
meta_key: 2019-uniter-learning-universal-image-text-representations
numCitedBy: 288
numCiting: 56
paperAbstract: 'Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are jointly processed for visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design three pre-training tasks: Masked Language Modeling (MLM), Image-Text Matching (ITM), and Masked Region Modeling (MRM, with three variants). Different from concurrent work on multimodal pre-training that apply joint random masking to both modalities, we use conditioned masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). Comprehensive analysis shows that conditioned masking yields better performance than unconditioned masking. We also conduct a thorough ablation study to find an optimal setting for the combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR2.'
ref_count: 56
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training
  numCitedBy: 394
  pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  show_ref_link: true
  title: Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-learning-deep-structure-preserving-image-text-embeddings
  numCitedBy: 587
  pid: b27e791e843c924ef052981b79490ab59fc0433d
  show_ref_link: true
  title: Learning Deep Structure-Preserving Image-Text Embeddings
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-stacked-cross-attention-for-image-text-matching
  numCitedBy: 491
  pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  show_ref_link: true
  title: Stacked Cross Attention for Image-Text Matching
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks
  numCitedBy: 1319
  pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  show_ref_link: true
  title: ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding
  numCitedBy: 1092
  pid: fddc15480d086629b960be5bff96232f967f2252
  show_ref_link: true
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-videobert-a-joint-model-for-video-and-language-representation-learning
  numCitedBy: 584
  pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  show_ref_link: true
  title: VideoBERT - A Joint Model for Video and Language Representation Learning
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-mattnet-modular-attention-network-for-referring-expression-comprehension
  numCitedBy: 374
  pid: fdce9cbe5c726201575b3c8a8c1af0752f1af53f
  show_ref_link: true
  title: MAttNet - Modular Attention Network for Referring Expression Comprehension
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-context-encoders-feature-learning-by-inpainting
  numCitedBy: 3369
  pid: 7d0effebfa4bed19b6ba41f3af5b7e5b6890de87
  show_ref_link: false
  title: Context Encoders - Feature Learning by Inpainting
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-deep-visual-semantic-alignments-for-generating-image-descriptions
  numCitedBy: 2619
  pid: 55e022fb7581bb9e1fce678d21fb25ffbb3fbb88
  show_ref_link: true
  title: Deep Visual-Semantic Alignments for Generating Image Descriptions
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-lxmert-learning-cross-modality-encoder-representations-from-transformers
  numCitedBy: 952
  pid: 79c93274429d6355959f1e4374c2147bb81ea649
  show_ref_link: true
  title: LXMERT - Learning Cross-Modality Encoder Representations from Transformers
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-12-in-1-multi-task-vision-and-language-representation-learning
  numCitedBy: 234
  pid: 6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9
  show_ref_link: true
  title: 12-in-1 - Multi-Task Vision and Language Representation Learning
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-deep-modular-co-attention-networks-for-visual-question-answering
  numCitedBy: 324
  pid: 8a1744da011375d711ed75fc2d160c6fdca2cf89
  show_ref_link: true
  title: Deep Modular Co-Attention Networks for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language
  numCitedBy: 664
  pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  show_ref_link: true
  title: VisualBERT - A Simple and Performant Baseline for Vision and Language
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-visual-entailment-a-novel-task-for-fine-grained-image-understanding
  numCitedBy: 100
  pid: 3c54b796cc10cb530f77caa4d18e1c80ac863822
  show_ref_link: false
  title: Visual Entailment - A Novel Task for Fine-Grained Image Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-vl-bert-pre-training-of-generic-visual-linguistic-representations
  numCitedBy: 735
  pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  show_ref_link: true
  title: VL-BERT - Pre-training of Generic Visual-Linguistic Representations
  year: 2020
- fieldsOfStudy:
  - Computer Science
  - Environmental Science
  meta_key: 2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning
  numCitedBy: 648
  pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  show_ref_link: true
  title: Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-unsupervised-visual-representation-learning-by-context-prediction
  numCitedBy: 1777
  pid: fc1b1c9364c58ec406f494dd944b609a6a038ba6
  show_ref_link: false
  title: Unsupervised Visual Representation Learning by Context Prediction
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-unsupervised-representation-learning-by-predicting-image-rotations
  numCitedBy: 1684
  pid: aab368284210c1bb917ec2d31b84588e3d2d7eb4
  show_ref_link: false
  title: Unsupervised Representation Learning by Predicting Image Rotations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-flickr30k-entities-collecting-region-to-phrase-correspondences-for-richer-image-to-sentence-models
  numCitedBy: 673
  pid: 0612745dbd292fc0a548a16d39cd73e127faedde
  show_ref_link: false
  title: Flickr30k Entities - Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-language-models-are-unsupervised-multitask-learners
  numCitedBy: 6429
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  show_ref_link: true
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-attention-is-all-you-need
  numCitedBy: 35966
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
  numCitedBy: 34640
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering
  numCitedBy: 2310
  pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  show_ref_link: true
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-unsupervised-learning-of-visual-representations-by-solving-jigsaw-puzzles
  numCitedBy: 1683
  pid: 2ec8f7e0257a07d3914322b36072d1bbcd58a1e0
  show_ref_link: false
  title: Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-fusion-of-detected-objects-in-text-for-visual-question-answering
  numCitedBy: 114
  pid: b82153bf85d5d1edd3f170aace830e5328ca9ed0
  show_ref_link: true
  title: Fusion of Detected Objects in Text for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-albert-a-lite-bert-for-self-supervised-learning-of-language-representations
  numCitedBy: 2772
  pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  show_ref_link: true
  title: ALBERT - A Lite BERT for Self-supervised Learning of Language Representations
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-bilinear-attention-networks
  numCitedBy: 418
  pid: a5d10341717c0519cf63151b496a6d2ed67aa05f
  show_ref_link: true
  title: Bilinear Attention Networks
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding
  numCitedBy: 4296
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  show_ref_link: true
  title: XLNet - Generalized Autoregressive Pretraining for Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations
  numCitedBy: 2810
  pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  show_ref_link: true
  title: Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-from-recognition-to-cognition-visual-commonsense-reasoning
  numCitedBy: 379
  pid: 6dfc2ff03534a4325d06c6f88c3144831996629b
  show_ref_link: true
  title: From Recognition to Cognition - Visual Commonsense Reasoning
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-colorful-image-colorization
  numCitedBy: 2318
  pid: 8201e6e687f2de477258e9be53ba7b73ee30d7de
  show_ref_link: false
  title: Colorful Image Colorization
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-learning-video-representations-using-contrastive-bidirectional-transformer
  numCitedBy: 171
  pid: 025a0dc4a2a98742f1b410b6318a46de2c854b22
  show_ref_link: false
  title: Learning Video Representations using Contrastive Bidirectional Transformer
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-deep-contextualized-word-representations
  numCitedBy: 8062
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-learning-generative-models-with-sinkhorn-divergences
  numCitedBy: 394
  pid: a1bc7d90564c342beb75cedf36fd921de89d94ad
  show_ref_link: false
  title: Learning Generative Models with Sinkhorn Divergences
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-modeling-context-in-referring-expressions
  numCitedBy: 384
  pid: 29efbe391950ae438c63d86ad5c82b2942efb0b4
  show_ref_link: true
  title: Modeling Context in Referring Expressions
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-nlvr2-visual-bias-analysis
  numCitedBy: 5
  pid: 8e86dd59429e8b7fd34b6893c2dea3921974c328
  show_ref_link: false
  title: NLVR2 Visual Bias Analysis
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-gqa-a-new-dataset-for-real-world-visual-reasoning-and-compositional-question-answering
  numCitedBy: 462
  pid: 1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1
  show_ref_link: true
  title: GQA - A New Dataset for Real-World Visual Reasoning and Compositional Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2014-microsoft-coco-common-objects-in-context
  numCitedBy: 19980
  pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  show_ref_link: true
  title: Microsoft COCO - Common Objects in Context
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-scaling-neural-machine-translation
  numCitedBy: 475
  pid: bf8fe437f779f2098f9af82b534aa51dc9edb06f
  show_ref_link: false
  title: Scaling Neural Machine Translation
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation
  numCitedBy: 4687
  pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  show_ref_link: true
  title: Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2011-im2text-describing-images-using-1-million-captioned-photographs
  numCitedBy: 741
  pid: 8e080b98efbe65c02a116439205ca2344b9f7cd4
  show_ref_link: false
  title: Im2Text - Describing Images Using 1 Million Captioned Photographs
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-distilling-the-knowledge-in-a-neural-network
  numCitedBy: 8798
  pid: 0c908739fbff75f03469d13d4a1a07de3414ee19
  show_ref_link: false
  title: Distilling the Knowledge in a Neural Network
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-roberta-a-robustly-optimized-bert-pretraining-approach
  numCitedBy: 7554
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  show_ref_link: true
  title: RoBERTa - A Robustly Optimized BERT Pretraining Approach
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-a-corpus-for-reasoning-about-natural-language-grounded-in-photographs
  numCitedBy: 214
  pid: cf336d272a30d6ad6141db67faa64deb8791cd61
  show_ref_link: true
  title: A Corpus for Reasoning about Natural Language Grounded in Photographs
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-revealing-the-dark-secrets-of-bert
  numCitedBy: 294
  pid: d78aed1dac6656affa4a04cbf225ced11a83d103
  show_ref_link: false
  title: Revealing the Dark Secrets of BERT
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-decoupled-weight-decay-regularization
  numCitedBy: 3595
  pid: d07284a6811f1b2745d91bdb06b040b57f226882
  show_ref_link: true
  title: Decoupled Weight Decay Regularization
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-improving-gans-using-optimal-transport
  numCitedBy: 210
  pid: 69902406e7d08f8865f02185699978db499d25e7
  show_ref_link: false
  title: Improving GANs Using Optimal Transport
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-automatic-differentiation-in-pytorch
  numCitedBy: 10406
  pid: b36a5bb1707bb9c70025294b3a310138aae8327a
  show_ref_link: true
  title: Automatic differentiation in PyTorch
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-a-fast-proximal-point-method-for-wasserstein-distance
  numCitedBy: 37
  pid: 7b54a851675cc73367cd28c296d393564ebe55f5
  show_ref_link: false
  title: A Fast Proximal Point Method for Wasserstein Distance
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-a-fast-proximal-point-method-for-computing-wasserstein-distance
  numCitedBy: 16
  pid: a2aafb1cdfdf657b3ee7336071d5a4cbae6385f7
  show_ref_link: false
  title: A Fast Proximal Point Method for Computing Wasserstein Distance
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2013-sinkhorn-distances-lightspeed-computation-of-optimal-transport
  numCitedBy: 1897
  pid: 0080118b0eb02af581ff32b85a1bb6aed7081f45
  show_ref_link: false
  title: Sinkhorn Distances - Lightspeed Computation of Optimal Transport
  year: 2013
- fieldsOfStudy:
  - Geology
  meta_key: 2019-computational-optimal-transport
  numCitedBy: 965
  pid: 8e51d68250db5637cd6bc1de98a99396441399b2
  show_ref_link: false
  title: Computational Optimal Transport
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2014-referitgame-referring-to-objects-in-photographs-of-natural-scenes
  numCitedBy: 567
  pid: 92c141447f51b6732242376164ff961e464731c8
  show_ref_link: false
  title: ReferItGame - Referring to Objects in Photographs of Natural Scenes
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-selfie-self-supervised-pretraining-for-image-embedding
  numCitedBy: 68
  pid: 5466ee5f16fc3c776fd1da667917592e5fd06720
  show_ref_link: false
  title: Selfie - Self-supervised Pretraining for Image Embedding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-wasserstein-generative-adversarial-networks
  numCitedBy: 3974
  pid: acd87843a451d18b4dc6474ddce1ae946429eaf1
  show_ref_link: false
  title: Wasserstein Generative Adversarial Networks
  year: 2017
slug: UNITER:-Learning-UNiversal-Image-TExt-Chen-Li
title: UNITER - Learning UNiversal Image-TExt Representations
url: https://www.semanticscholar.org/paper/UNITER:-Learning-UNiversal-Image-TExt-Chen-Li/54416048772b921720f19869ed11c2a360589d03?sort=total-citations
venue: ECCV 2020
year: 2019
