authors:
- Zhe Gan
- Yen-Chun Chen
- Linjie Li
- Chen Zhu
- Yu Cheng
- Jingjing Liu
badges:
- id: OPEN_ACCESS
corpusId: 219573512
fieldsOfStudy:
- Computer Science
meta_key: 2020-large-scale-adversarial-training-for-vision-and-language-representation-learning
numCitedBy: 173
numCiting: 93
paperAbstract: 'We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the "free" adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.'
ref_count: 93
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-adversarial-training-for-large-neural-language-models
  numCitedBy: 75
  pid: 2ffcf8352223c95ae8cef4daaec995525ecc926b
  show_ref_link: false
  title: Adversarial Training for Large Neural Language Models
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-freelb-enhanced-adversarial-training-for-language-understanding
  numCitedBy: 64
  pid: d2038ced371e45aee3651c7a595c4566f4826b9f
  show_ref_link: false
  title: FreeLB - Enhanced Adversarial Training for Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-behind-the-scene-revealing-the-secrets-of-pre-trained-vision-and-language-models
  numCitedBy: 67
  pid: 26cfb57a9722599b361858d454ec816420723e36
  show_ref_link: false
  title: Behind the Scene - Revealing the Secrets of Pre-trained Vision-and-Language Models
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-improving-neural-language-modeling-via-adversarial-training
  numCitedBy: 66
  pid: e84d754564c9e2ce993596370e0a1493c9c6e4b1
  show_ref_link: false
  title: Improving Neural Language Modeling via Adversarial Training
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-overcoming-language-priors-in-visual-question-answering-with-adversarial-regularization
  numCitedBy: 131
  pid: 45ec1446f42c0a7c7fe74319118335c76e0f7b19
  show_ref_link: false
  title: Overcoming Language Priors in Visual Question Answering with Adversarial Regularization
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-adversarial-training-methods-for-semi-supervised-text-classification
  numCitedBy: 589
  pid: 2cd55ded95d5d13430edfa223ba591b514ebe8a5
  show_ref_link: false
  title: Adversarial Training Methods for Semi-Supervised Text Classification
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-uniter-learning-universal-image-text-representations
  numCitedBy: 288
  pid: 54416048772b921720f19869ed11c2a360589d03
  show_ref_link: true
  title: UNITER - Learning UNiversal Image-TExt Representations
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-using-pre-training-can-improve-model-robustness-and-uncertainty
  numCitedBy: 349
  pid: aa5741c74b7fac10680c1cfbdd49d9ffb5751a68
  show_ref_link: false
  title: Using Pre-Training Can Improve Model Robustness and Uncertainty
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-you-only-propagate-once-accelerating-adversarial-training-via-maximal-principle
  numCitedBy: 204
  pid: d33deae7f654b07ac8a5c437a4fa018c29e6af17
  show_ref_link: false
  title: You Only Propagate Once - Accelerating Adversarial Training via Maximal Principle
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-virtual-adversarial-training-a-regularization-method-for-supervised-and-semi-supervised-learning
  numCitedBy: 1467
  pid: 4b1c6f6521da545892f3f5dc39461584d4a27ec0
  show_ref_link: false
  title: Virtual Adversarial Training - A Regularization Method for Supervised and Semi-Supervised Learning
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-attacking-visual-language-grounding-with-adversarial-examples-a-case-study-on-neural-image-captioning
  numCitedBy: 84
  pid: 77685c77a1fa39890006fe13f43738aac49a2c51
  show_ref_link: false
  title: Attacking Visual Language Grounding with Adversarial Examples - A Case Study on Neural Image Captioning
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training
  numCitedBy: 394
  pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  show_ref_link: true
  title: Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-adversarial-robustness-from-self-supervised-pre-training-to-fine-tuning
  numCitedBy: 118
  pid: 962a8ffc7d72990a28d505f49a39108b4803c223
  show_ref_link: false
  title: Adversarial Robustness - From Self-Supervised Pre-Training to Fine-Tuning
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks
  numCitedBy: 1319
  pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  show_ref_link: true
  title: ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-unified-vision-language-pre-training-for-image-captioning-and-vqa
  numCitedBy: 363
  pid: 6648b4db5f12c30941ea78c695e77aded19672bb
  show_ref_link: true
  title: Unified Vision-Language Pre-Training for Image Captioning and VQA
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-adversarial-training-for-free
  numCitedBy: 540
  pid: c92be891c5f8f0f60b6de206364f9a744612d1e8
  show_ref_link: false
  title: Adversarial Training for Free!
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-videobert-a-joint-model-for-video-and-language-representation-learning
  numCitedBy: 584
  pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  show_ref_link: true
  title: VideoBERT - A Joint Model for Video and Language Representation Learning
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-adversarial-examples-improve-image-recognition
  numCitedBy: 263
  pid: 948839277bface5780896e8e8791906818aa41ac
  show_ref_link: false
  title: Adversarial Examples Improve Image Recognition
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-boosting-adversarial-training-with-hypersphere-embedding
  numCitedBy: 67
  pid: d5b84236178d7805c2e7b503cc6cf4a24b7da626
  show_ref_link: false
  title: Boosting Adversarial Training with Hypersphere Embedding
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-lxmert-learning-cross-modality-encoder-representations-from-transformers
  numCitedBy: 952
  pid: 79c93274429d6355959f1e4374c2147bb81ea649
  show_ref_link: true
  title: LXMERT - Learning Cross-Modality Encoder Representations from Transformers
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-large-scale-pretraining-for-visual-dialog-a-simple-state-of-the-art-baseline
  numCitedBy: 57
  pid: 21d4d97fed755a9ecd5b67a42e1c69008989738c
  show_ref_link: false
  title: Large-scale Pretraining for Visual Dialog - A Simple State-of-the-Art Baseline
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-metric-learning-for-adversarial-robustness
  numCitedBy: 92
  pid: ead9e59097c2839bf628c7382a53ac7fcea60f14
  show_ref_link: false
  title: Metric Learning for Adversarial Robustness
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-improving-the-robustness-of-deep-neural-networks-via-adversarial-training-with-triplet-loss
  numCitedBy: 21
  pid: bc696c8b7b507b9e2c72d48b1ba1abaa62bd705d
  show_ref_link: false
  title: Improving the Robustness of Deep Neural Networks via Adversarial Training with Triplet Loss
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-12-in-1-multi-task-vision-and-language-representation-learning
  numCitedBy: 234
  pid: 6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9
  show_ref_link: true
  title: 12-in-1 - Multi-Task Vision and Language Representation Learning
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-feature-denoising-for-improving-adversarial-robustness
  numCitedBy: 543
  pid: 41071dbbbcbb27af3fec70de045f19c28535f5b7
  show_ref_link: false
  title: Feature Denoising for Improving Adversarial Robustness
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-oscar-object-semantics-aligned-pre-training-for-vision-language-tasks
  numCitedBy: 562
  pid: 818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57
  show_ref_link: true
  title: Oscar - Object-Semantics Aligned Pre-training for Vision-Language Tasks
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-fast-is-better-than-free-revisiting-adversarial-training
  numCitedBy: 479
  pid: 6d4a87759917132913319960389f17fa1fe8b630
  show_ref_link: false
  title: Fast is better than free - Revisiting adversarial training
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-are-labels-required-for-improving-adversarial-robustness
  numCitedBy: 195
  pid: 6d12401822a24b2ff5542a7fa72158d891960c62
  show_ref_link: false
  title: Are Labels Required for Improving Adversarial Robustness?
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
  numCitedBy: 34640
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-exact-adversarial-attack-to-image-captioning-via-structured-output-learning-with-latent-variables
  numCitedBy: 29
  pid: aacdfd9fd9bf59afdc6612678440581f229d270e
  show_ref_link: false
  title: Exact Adversarial Attack to Image Captioning via Structured Output Learning With Latent Variables
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-multi-modal-factorized-bilinear-pooling-with-co-attention-learning-for-visual-question-answering
  numCitedBy: 369
  pid: 8e9ad6f8b2bc97f0412fa0cc243ac6975864534a
  show_ref_link: false
  title: Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-ensemble-adversarial-training-attacks-and-defenses
  numCitedBy: 1710
  pid: 136dee73f203df2f4831994bf4f0c0a4ad2e764e
  show_ref_link: false
  title: Ensemble Adversarial Training - Attacks and Defenses
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-towards-learning-a-generic-agent-for-vision-and-language-navigation-via-pre-training
  numCitedBy: 97
  pid: 6fa25c94e41a0c90e3aabe80cf60f59ec9ff0a52
  show_ref_link: false
  title: Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-vd-bert-a-unified-vision-and-dialog-transformer-with-bert
  numCitedBy: 48
  pid: 06c7269c10125589d2599f684b751b1640f7a0cc
  show_ref_link: false
  title: VD-BERT - A Unified Vision and Dialog Transformer with BERT
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-explaining-and-harnessing-adversarial-examples
  numCitedBy: 10200
  pid: bee044c8e8903fb67523c1f8c105ab4718600cdb
  show_ref_link: false
  title: Explaining and Harnessing Adversarial Examples
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-from-recognition-to-cognition-visual-commonsense-reasoning
  numCitedBy: 379
  pid: 6dfc2ff03534a4325d06c6f88c3144831996629b
  show_ref_link: true
  title: From Recognition to Cognition - Visual Commonsense Reasoning
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-unlabeled-data-improves-adversarial-robustness
  numCitedBy: 381
  pid: b3f1aa12dde233aaf543bb9ccb27213c494e0fd5
  show_ref_link: false
  title: Unlabeled Data Improves Adversarial Robustness
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language
  numCitedBy: 664
  pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  show_ref_link: true
  title: VisualBERT - A Simple and Performant Baseline for Vision and Language
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering
  numCitedBy: 1178
  pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  show_ref_link: true
  title: Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-defense-against-adversarial-attacks-using-feature-scattering-based-adversarial-training
  numCitedBy: 121
  pid: 2c1006c856fefdbd6cd710e840e57153f2d6cd04
  show_ref_link: false
  title: Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-vl-bert-pre-training-of-generic-visual-linguistic-representations
  numCitedBy: 735
  pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  show_ref_link: true
  title: VL-BERT - Pre-training of Generic Visual-Linguistic Representations
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-visual-entailment-a-novel-task-for-fine-grained-image-understanding
  numCitedBy: 100
  pid: 3c54b796cc10cb530f77caa4d18e1c80ac863822
  show_ref_link: false
  title: Visual Entailment - A Novel Task for Fine-Grained Image Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2021-xgpt-cross-modal-generative-pre-training-for-image-captioning
  numCitedBy: 33
  pid: 1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0
  show_ref_link: false
  title: XGPT - Cross-modal Generative Pre-Training for Image Captioning
  year: 2021
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-hero-hierarchical-encoder-for-video-language-omni-representation-pre-training
  numCitedBy: 135
  pid: 5546e6073f3b82967b12c87d6b90ba722c4b85c6
  show_ref_link: false
  title: Hero - Hierarchical Encoder for Video+Language Omni-representation Pre-training
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-towards-deep-learning-models-resistant-to-adversarial-attacks
  numCitedBy: 5482
  pid: 7aa38b85fa8cba64d6a4010543f6695dbf5f1386
  show_ref_link: false
  title: Towards Deep Learning Models Resistant to Adversarial Attacks
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding
  numCitedBy: 1092
  pid: fddc15480d086629b960be5bff96232f967f2252
  show_ref_link: true
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-deep-modular-co-attention-networks-for-visual-question-answering
  numCitedBy: 324
  pid: 8a1744da011375d711ed75fc2d160c6fdca2cf89
  show_ref_link: true
  title: Deep Modular Co-Attention Networks for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-learning-conditioned-graph-structures-for-interpretable-visual-question-answering
  numCitedBy: 152
  pid: 6ac33d3dcecbed17580509a34bccdff2425f7ed8
  show_ref_link: false
  title: Learning Conditioned Graph Structures for Interpretable Visual Question Answering
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-attention-is-all-you-need
  numCitedBy: 35966
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-unsupervised-data-augmentation-for-consistency-training
  numCitedBy: 878
  pid: 0feea94f89d395436bf41bd10c797447eecbc128
  show_ref_link: false
  title: Unsupervised Data Augmentation for Consistency Training
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-neural-module-networks
  numCitedBy: 744
  pid: 21c99706bb26e9012bfb4d8d48009a3d45af59b2
  show_ref_link: true
  title: Neural Module Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-contrastive-bidirectional-transformer-for-temporal-representation-learning
  numCitedBy: 117
  pid: f259bc7ef31c4ec7dd041c94bfd6b2f93b99b47c
  show_ref_link: false
  title: Contrastive Bidirectional Transformer for Temporal Representation Learning
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-smart-robust-and-efficient-fine-tuning-for-pre-trained-natural-language-models-through-principled-regularized-optimization
  numCitedBy: 166
  pid: ab70853cd5912c470f6ff95e95481980f0a2a41b
  show_ref_link: false
  title: SMART - Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-pixel-bert-aligning-image-pixels-with-text-by-deep-multi-modal-transformers
  numCitedBy: 155
  pid: 598a2ee223e2949c3b28389e922c1892b4717d2a
  show_ref_link: false
  title: Pixel-BERT - Aligning Image Pixels with Text by Deep Multi-Modal Transformers
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-hierarchical-question-image-co-attention-for-visual-question-answering
  numCitedBy: 1130
  pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  show_ref_link: true
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-univilm-a-unified-video-and-language-pre-training-model-for-multimodal-understanding-and-generation
  numCitedBy: 115
  pid: 4243555758433880a67b15b50f752b1e2a8c4609
  show_ref_link: false
  title: UniViLM - A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-murel-multimodal-relational-reasoning-for-visual-question-answering
  numCitedBy: 171
  pid: 0a9f1a1321958df7dfb2efce3e9d1e99b9f5ccb3
  show_ref_link: false
  title: MUREL - Multimodal Relational Reasoning for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-fusion-of-detected-objects-in-text-for-visual-question-answering
  numCitedBy: 114
  pid: b82153bf85d5d1edd3f170aace830e5328ca9ed0
  show_ref_link: true
  title: Fusion of Detected Objects in Text for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-semantically-equivalent-adversarial-rules-for-debugging-nlp-models
  numCitedBy: 329
  pid: 472644c5f4155635cf9e9e37540bfa53c20e7610
  show_ref_link: false
  title: Semantically Equivalent Adversarial Rules for Debugging NLP models
  year: 2018
- fieldsOfStudy:
  - Psychology
  meta_key: "2020-5\u5206\u3067\u5206\u304B\u308B-\u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\u8AAD\u307F-jacob-devlin-et-al-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"
  numCitedBy: 1023
  pid: 43f2ad297941db230c089ba353efc3f281ab678c
  show_ref_link: false
  title: "5\u5206\u3067\u5206\u304B\u308B!? \u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\u8AAD\u307F\uFF1AJacob Devlin et al. - BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding"
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-gqa-a-new-dataset-for-real-world-visual-reasoning-and-compositional-question-answering
  numCitedBy: 462
  pid: 1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1
  show_ref_link: true
  title: GQA - A New Dataset for Real-World Visual Reasoning and Compositional Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-bilinear-attention-networks
  numCitedBy: 418
  pid: a5d10341717c0519cf63151b496a6d2ed67aa05f
  show_ref_link: true
  title: Bilinear Attention Networks
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-learning-to-reason-end-to-end-module-networks-for-visual-question-answering
  numCitedBy: 435
  pid: a396a6febdacb84340d139096455e67049ac1e22
  show_ref_link: true
  title: Learning to Reason - End-to-End Module Networks for Visual Question Answering
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations
  numCitedBy: 2810
  pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  show_ref_link: true
  title: Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-relation-aware-graph-attention-network-for-visual-question-answering
  numCitedBy: 147
  pid: d379ba96b8f400b23b2cd72c428af67e578959ea
  show_ref_link: false
  title: Relation-Aware Graph Attention Network for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2014-intriguing-properties-of-neural-networks
  numCitedBy: 8941
  pid: d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad
  show_ref_link: false
  title: Intriguing properties of neural networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-compositional-attention-networks-for-machine-reasoning
  numCitedBy: 408
  pid: 289fb3709475f5c87df8d97f129af54029d27fee
  show_ref_link: true
  title: Compositional Attention Networks for Machine Reasoning
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-improving-vision-and-language-navigation-with-image-text-pairs-from-the-web
  numCitedBy: 83
  pid: d1ac487f21829ef56c8ffdcd37ea414bce68c809
  show_ref_link: false
  title: Improving Vision-and-Language Navigation with Image-Text Pairs from the Web
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-flickr30k-entities-collecting-region-to-phrase-correspondences-for-richer-image-to-sentence-models
  numCitedBy: 673
  pid: 0612745dbd292fc0a548a16d39cd73e127faedde
  show_ref_link: false
  title: Flickr30k Entities - Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
  year: 2015
- fieldsOfStudy:
  - Psychology
  meta_key: "2020-5\u5206\u3067\u5206\u304B\u308B-\u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\u8AAD\u307F-jacob-devlin-et-al-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"
  numCitedBy: 1023
  pid: 43f2ad297941db230c089ba353efc3f281ab678c
  show_ref_link: false
  title: "5\u5206\u3067\u5206\u304B\u308B!? \u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\u8AAD\u307F\uFF1AJacob Devlin et al. - BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding"
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-a-simple-neural-network-module-for-relational-reasoning
  numCitedBy: 1205
  pid: 007112213ece771be72cbecfd59f048209facabd
  show_ref_link: true
  title: A simple neural network module for relational reasoning
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-stacked-cross-attention-for-image-text-matching
  numCitedBy: 491
  pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  show_ref_link: true
  title: Stacked Cross Attention for Image-Text Matching
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-dynamic-fusion-with-intra-and-inter-modality-attention-flow-for-visual-question-answering
  numCitedBy: 191
  pid: e9b13731027418ed38103d1dfc8a70f6881bc684
  show_ref_link: true
  title: Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-stacked-attention-networks-for-image-question-answering
  numCitedBy: 1480
  pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  show_ref_link: true
  title: Stacked Attention Networks for Image Question Answering
  year: 2016
- fieldsOfStudy:
  - Computer Science
  - Environmental Science
  meta_key: 2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning
  numCitedBy: 648
  pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  show_ref_link: true
  title: Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-modeling-context-in-referring-expressions
  numCitedBy: 384
  pid: 29efbe391950ae438c63d86ad5c82b2942efb0b4
  show_ref_link: true
  title: Modeling Context in Referring Expressions
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-multi-step-reasoning-via-recurrent-dual-attention-for-visual-dialog
  numCitedBy: 72
  pid: 8d9f1eaac344b7e6cc76396f576c5dc0bd0b34f4
  show_ref_link: false
  title: Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2021-meta-module-network-for-compositional-visual-reasoning
  numCitedBy: 33
  pid: 320464aa0231bc728c7d9ab7e71e552c12a7486b
  show_ref_link: false
  title: Meta Module Network for Compositional Visual Reasoning
  year: 2021
- fieldsOfStudy:
  - Computer Science
  meta_key: 2014-microsoft-coco-common-objects-in-context
  numCitedBy: 19980
  pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  show_ref_link: true
  title: Microsoft COCO - Common Objects in Context
  year: 2014
- fieldsOfStudy:
  - Psychology
  meta_key: "2020-5\u5206\u3067\u5206\u304B\u308B-\u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\u8AAD\u307F-jacob-devlin-et-al-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"
  numCitedBy: 1023
  pid: 43f2ad297941db230c089ba353efc3f281ab678c
  show_ref_link: false
  title: "5\u5206\u3067\u5206\u304B\u308B!? \u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\u8AAD\u307F\uFF1AJacob Devlin et al. - BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding"
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2011-im2text-describing-images-using-1-million-captioned-photographs
  numCitedBy: 741
  pid: 8e080b98efbe65c02a116439205ca2344b9f7cd4
  show_ref_link: false
  title: Im2Text - Describing Images Using 1 Million Captioned Photographs
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-inferring-and-executing-programs-for-visual-reasoning
  numCitedBy: 417
  pid: 2e17cf6a339fd071ad222062f868e882ef4120a4
  show_ref_link: true
  title: Inferring and Executing Programs for Visual Reasoning
  year: 2017
- fieldsOfStudy:
  - Computer Science
  - Mathematics
  meta_key: 2018-obfuscated-gradients-give-a-false-sense-of-security-circumventing-defenses-to-adversarial-examples
  numCitedBy: 2068
  pid: 651adaa058f821a890f2c5d1053d69eb481a8352
  show_ref_link: false
  title: Obfuscated Gradients Give a False Sense of Security - Circumventing Defenses to Adversarial Examples
  year: 2018
- fieldsOfStudy:
  - Psychology
  meta_key: "2020-5\u5206\u3067\u5206\u304B\u308B-\u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\u8AAD\u307F-jacob-devlin-et-al-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"
  numCitedBy: 1023
  pid: 43f2ad297941db230c089ba353efc3f281ab678c
  show_ref_link: false
  title: "5\u5206\u3067\u5206\u304B\u308B!? \u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\u8AAD\u307F\uFF1AJacob Devlin et al. - BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding"
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-uniter-universal-image-text-representation-learning
  numCitedBy: 608
  pid: d8a305b9366608d54452ac30459ee57b4f5cf1c9
  show_ref_link: true
  title: UNITER - UNiversal Image-TExt Representation Learning
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-learning-video-representations-using-contrastive-bidirectional-transformer
  numCitedBy: 171
  pid: 025a0dc4a2a98742f1b410b6318a46de2c854b22
  show_ref_link: false
  title: Learning Video Representations using Contrastive Bidirectional Transformer
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-freelb-enhanced-adversarial-training-for-natural-language-understanding
  numCitedBy: 176
  pid: d01fa0311e8e15b8b874b376123530c815f52852
  show_ref_link: false
  title: FreeLB - Enhanced Adversarial Training for Natural Language Understanding
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-cycle-consistency-for-robust-visual-question-answering
  numCitedBy: 94
  pid: 735a63b58349e07b84c2e31927ce1b1cfaf09980
  show_ref_link: false
  title: Cycle-Consistency for Robust Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-show-and-fool-crafting-adversarial-examples-for-neural-image-captioning
  numCitedBy: 43
  pid: 58d190282ed59639b16e726a3237938b53976077
  show_ref_link: false
  title: Show-and-Fool - Crafting Adversarial Examples for Neural Image Captioning
  year: 2017
slug: Large-Scale-Adversarial-Training-for-Representation-Gan-Chen
title: Large-Scale Adversarial Training for Vision-and-Language Representation Learning
url: https://www.semanticscholar.org/paper/Large-Scale-Adversarial-Training-for-Representation-Gan-Chen/2f5f81bc516a6d085d39479378af1fc27104f91e?sort=total-citations
venue: NeurIPS
year: 2020
