authors:
- Yen-Chun Chen
- Linjie Li
- Licheng Yu
- Ahmed El Kholy
- Faisal Ahmed
- Zhe Gan
- Yu Cheng
- Jingjing Liu
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 216080982
fieldsOfStudy:
- Computer Science
meta_key: 2020-uniter-universal-image-text-representation-learning
numCitedBy: 608
numCiting: 60
paperAbstract: 'Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR$^2$. Code is available at this https URL.'
ref_count: 59
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-unified-vision-language-pre-training-for-image-captioning-and-vqa
  numCitedBy: 363
  pid: 6648b4db5f12c30941ea78c695e77aded19672bb
  show_ref_link: true
  title: Unified Vision-Language Pre-Training for Image Captioning and VQA
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-stacked-cross-attention-for-image-text-matching
  numCitedBy: 491
  pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  show_ref_link: true
  title: Stacked Cross Attention for Image-Text Matching
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-behind-the-scene-revealing-the-secrets-of-pre-trained-vision-and-language-models
  numCitedBy: 67
  pid: 26cfb57a9722599b361858d454ec816420723e36
  show_ref_link: false
  title: Behind the Scene - Revealing the Secrets of Pre-trained Vision-and-Language Models
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training
  numCitedBy: 394
  pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  show_ref_link: true
  title: Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-learning-deep-structure-preserving-image-text-embeddings
  numCitedBy: 587
  pid: b27e791e843c924ef052981b79490ab59fc0433d
  show_ref_link: true
  title: Learning Deep Structure-Preserving Image-Text Embeddings
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-mattnet-modular-attention-network-for-referring-expression-comprehension
  numCitedBy: 374
  pid: fdce9cbe5c726201575b3c8a8c1af0752f1af53f
  show_ref_link: true
  title: MAttNet - Modular Attention Network for Referring Expression Comprehension
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks
  numCitedBy: 1319
  pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  show_ref_link: true
  title: ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-large-scale-adversarial-training-for-vision-and-language-representation-learning
  numCitedBy: 173
  pid: 2f5f81bc516a6d085d39479378af1fc27104f91e
  show_ref_link: true
  title: Large-Scale Adversarial Training for Vision-and-Language Representation Learning
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding
  numCitedBy: 1092
  pid: fddc15480d086629b960be5bff96232f967f2252
  show_ref_link: true
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-videobert-a-joint-model-for-video-and-language-representation-learning
  numCitedBy: 584
  pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  show_ref_link: true
  title: VideoBERT - A Joint Model for Video and Language Representation Learning
  year: 2019
- fieldsOfStudy:
  - Computer Science
  - Environmental Science
  meta_key: 2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning
  numCitedBy: 648
  pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  show_ref_link: true
  title: Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-context-encoders-feature-learning-by-inpainting
  numCitedBy: 3369
  pid: 7d0effebfa4bed19b6ba41f3af5b7e5b6890de87
  show_ref_link: false
  title: Context Encoders - Feature Learning by Inpainting
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-selfie-self-supervised-pretraining-for-image-embedding
  numCitedBy: 68
  pid: 5466ee5f16fc3c776fd1da667917592e5fd06720
  show_ref_link: false
  title: Selfie - Self-supervised Pretraining for Image Embedding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-12-in-1-multi-task-vision-and-language-representation-learning
  numCitedBy: 234
  pid: 6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9
  show_ref_link: true
  title: 12-in-1 - Multi-Task Vision and Language Representation Learning
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-lxmert-learning-cross-modality-encoder-representations-from-transformers
  numCitedBy: 952
  pid: 79c93274429d6355959f1e4374c2147bb81ea649
  show_ref_link: true
  title: LXMERT - Learning Cross-Modality Encoder Representations from Transformers
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language
  numCitedBy: 664
  pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  show_ref_link: true
  title: VisualBERT - A Simple and Performant Baseline for Vision and Language
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-visual-entailment-a-novel-task-for-fine-grained-image-understanding
  numCitedBy: 100
  pid: 3c54b796cc10cb530f77caa4d18e1c80ac863822
  show_ref_link: false
  title: Visual Entailment - A Novel Task for Fine-Grained Image Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-vl-bert-pre-training-of-generic-visual-linguistic-representations
  numCitedBy: 735
  pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  show_ref_link: true
  title: VL-BERT - Pre-training of Generic Visual-Linguistic Representations
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-fusion-of-detected-objects-in-text-for-visual-question-answering
  numCitedBy: 114
  pid: b82153bf85d5d1edd3f170aace830e5328ca9ed0
  show_ref_link: true
  title: Fusion of Detected Objects in Text for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-graph-optimal-transport-for-cross-domain-alignment
  numCitedBy: 45
  pid: 2a81f6bf76bcb70244aa40217ff316025971bd0f
  show_ref_link: false
  title: Graph Optimal Transport for Cross-Domain Alignment
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering
  numCitedBy: 2310
  pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  show_ref_link: true
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-unsupervised-representation-learning-by-predicting-image-rotations
  numCitedBy: 1684
  pid: aab368284210c1bb917ec2d31b84588e3d2d7eb4
  show_ref_link: false
  title: Unsupervised Representation Learning by Predicting Image Rotations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-language-models-are-unsupervised-multitask-learners
  numCitedBy: 6429
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  show_ref_link: true
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-attention-is-all-you-need
  numCitedBy: 35966
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-unsupervised-learning-of-visual-representations-by-solving-jigsaw-puzzles
  numCitedBy: 1683
  pid: 2ec8f7e0257a07d3914322b36072d1bbcd58a1e0
  show_ref_link: false
  title: Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-contrastive-bidirectional-transformer-for-temporal-representation-learning
  numCitedBy: 117
  pid: f259bc7ef31c4ec7dd041c94bfd6b2f93b99b47c
  show_ref_link: false
  title: Contrastive Bidirectional Transformer for Temporal Representation Learning
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-colorful-image-colorization
  numCitedBy: 2322
  pid: 8201e6e687f2de477258e9be53ba7b73ee30d7de
  show_ref_link: false
  title: Colorful Image Colorization
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2020-albert-a-lite-bert-for-self-supervised-learning-of-language-representations
  numCitedBy: 2773
  pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  show_ref_link: true
  title: ALBERT - A Lite BERT for Self-supervised Learning of Language Representations
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-vqa-visual-question-answering
  numCitedBy: 2930
  pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  show_ref_link: true
  title: VQA - Visual Question Answering
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-learning-video-representations-using-contrastive-bidirectional-transformer
  numCitedBy: 171
  pid: 025a0dc4a2a98742f1b410b6318a46de2c854b22
  show_ref_link: false
  title: Learning Video Representations using Contrastive Bidirectional Transformer
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-bilinear-attention-networks
  numCitedBy: 418
  pid: a5d10341717c0519cf63151b496a6d2ed67aa05f
  show_ref_link: true
  title: Bilinear Attention Networks
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-from-recognition-to-cognition-visual-commonsense-reasoning
  numCitedBy: 379
  pid: 6dfc2ff03534a4325d06c6f88c3144831996629b
  show_ref_link: true
  title: From Recognition to Cognition - Visual Commonsense Reasoning
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding
  numCitedBy: 4296
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  show_ref_link: true
  title: XLNet - Generalized Autoregressive Pretraining for Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-deep-contextualized-word-representations
  numCitedBy: 8062
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-dynamic-fusion-with-intra-and-inter-modality-attention-flow-for-visual-question-answering
  numCitedBy: 191
  pid: e9b13731027418ed38103d1dfc8a70f6881bc684
  show_ref_link: true
  title: Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2011-im2text-describing-images-using-1-million-captioned-photographs
  numCitedBy: 741
  pid: 8e080b98efbe65c02a116439205ca2344b9f7cd4
  show_ref_link: false
  title: Im2Text - Describing Images Using 1 Million Captioned Photographs
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation
  numCitedBy: 4687
  pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  show_ref_link: true
  title: Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-nlvr2-visual-bias-analysis
  numCitedBy: 5
  pid: 8e86dd59429e8b7fd34b6893c2dea3921974c328
  show_ref_link: false
  title: NLVR2 Visual Bias Analysis
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-learning-generative-models-with-sinkhorn-divergences
  numCitedBy: 394
  pid: a1bc7d90564c342beb75cedf36fd921de89d94ad
  show_ref_link: false
  title: Learning Generative Models with Sinkhorn Divergences
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-scaling-neural-machine-translation
  numCitedBy: 475
  pid: bf8fe437f779f2098f9af82b534aa51dc9edb06f
  show_ref_link: false
  title: Scaling Neural Machine Translation
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-a-corpus-for-reasoning-about-natural-language-grounded-in-photographs
  numCitedBy: 214
  pid: cf336d272a30d6ad6141db67faa64deb8791cd61
  show_ref_link: true
  title: A Corpus for Reasoning about Natural Language Grounded in Photographs
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-distilling-the-knowledge-in-a-neural-network
  numCitedBy: 8808
  pid: 0c908739fbff75f03469d13d4a1a07de3414ee19
  show_ref_link: true
  title: Distilling the Knowledge in a Neural Network
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-automatic-differentiation-in-pytorch
  numCitedBy: 10406
  pid: b36a5bb1707bb9c70025294b3a310138aae8327a
  show_ref_link: true
  title: Automatic differentiation in PyTorch
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-a-fast-proximal-point-method-for-wasserstein-distance
  numCitedBy: 37
  pid: 7b54a851675cc73367cd28c296d393564ebe55f5
  show_ref_link: false
  title: A Fast Proximal Point Method for Wasserstein Distance
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-decoupled-weight-decay-regularization
  numCitedBy: 3608
  pid: d07284a6811f1b2745d91bdb06b040b57f226882
  show_ref_link: true
  title: Decoupled Weight Decay Regularization
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-improving-gans-using-optimal-transport
  numCitedBy: 210
  pid: 69902406e7d08f8865f02185699978db499d25e7
  show_ref_link: false
  title: Improving GANs Using Optimal Transport
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2019-revealing-the-dark-secrets-of-bert
  numCitedBy: 294
  pid: d78aed1dac6656affa4a04cbf225ced11a83d103
  show_ref_link: false
  title: Revealing the Dark Secrets of BERT
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-wasserstein-generative-adversarial-networks
  numCitedBy: 3975
  pid: acd87843a451d18b4dc6474ddce1ae946429eaf1
  show_ref_link: false
  title: Wasserstein Generative Adversarial Networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2013-sinkhorn-distances-lightspeed-computation-of-optimal-transport
  numCitedBy: 1899
  pid: 0080118b0eb02af581ff32b85a1bb6aed7081f45
  show_ref_link: false
  title: Sinkhorn Distances - Lightspeed Computation of Optimal Transport
  year: 2013
- fieldsOfStudy:
  - Geology
  meta_key: 2019-computational-optimal-transport
  numCitedBy: 967
  pid: 8e51d68250db5637cd6bc1de98a99396441399b2
  show_ref_link: false
  title: Computational Optimal Transport
  year: 2019
slug: UNITER:-UNiversal-Image-TExt-Representation-Chen-Li
title: UNITER - UNiversal Image-TExt Representation Learning
url: https://www.semanticscholar.org/paper/UNITER:-UNiversal-Image-TExt-Representation-Chen-Li/d8a305b9366608d54452ac30459ee57b4f5cf1c9?sort=total-citations
venue: ECCV
year: 2020
