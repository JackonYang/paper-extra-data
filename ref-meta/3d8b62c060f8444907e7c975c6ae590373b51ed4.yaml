authors:
- Raghuraman Krishnamoorthi
badges:
- id: OPEN_ACCESS
corpusId: 49356451
fieldsOfStudy:
- Computer Science
meta_key: 2018-quantizing-deep-convolutional-networks-for-efficient-inference-a-whitepaper
numCitedBy: 450
numCiting: 32
paperAbstract: "We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. \nQuantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits."
ref_count: 32
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-quantization-and-training-of-neural-networks-for-efficient-integer-arithmetic-only-inference
  numCitedBy: 1284
  pid: 59d0d7ccec2db66cad20cac5721ce54a8a058294
  show_ref_link: true
  title: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding
  numCitedBy: 5732
  pid: 642d0f49b7826adcf986616f4af77e736229990f
  show_ref_link: true
  title: Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations
  numCitedBy: 2134
  pid: a5733ff08daff727af834345b9cfff1d0aa109ec
  show_ref_link: true
  title: BinaryConnect - Training Deep Neural Networks with binary weights during propagations
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-model-compression-via-distillation-and-quantization
  numCitedBy: 389
  pid: f6a4bf043af1a9ec7f104a7b7ab56806b241ceda
  show_ref_link: false
  title: Model compression via distillation and quantization
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-eie-efficient-inference-engine-on-compressed-deep-neural-network
  numCitedBy: 1816
  pid: 2e2b189f668cf2c06ebc44dc9b166648256cf457
  show_ref_link: true
  title: EIE - Efficient Inference Engine on Compressed Deep Neural Network
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-squeezenet-alexnet-level-accuracy-with-50x-fewer-parameters-and-1mb-model-size
  numCitedBy: 4092
  pid: 969fbdcd0717bec06228053788c2ff78bbb4daac
  show_ref_link: true
  title: SqueezeNet - AlexNet-level accuracy with 50x fewer parameters and <1MB model size
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-a-quantization-friendly-separable-convolution-for-mobilenets
  numCitedBy: 90
  pid: c9b61aab2ce451612b1df245af80efd480dd3468
  show_ref_link: false
  title: A Quantization-Friendly Separable Convolution for MobileNets
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-efficient-processing-of-deep-neural-networks-a-tutorial-and-survey
  numCitedBy: 1748
  pid: 3f116042f50a499ab794bcc1255915bee507413c
  show_ref_link: false
  title: Efficient Processing of Deep Neural Networks - A Tutorial and Survey
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-rethinking-the-inception-architecture-for-computer-vision
  numCitedBy: 15857
  pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  show_ref_link: true
  title: Rethinking the Inception Architecture for Computer Vision
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-adc-automated-deep-compression-and-acceleration-with-reinforcement-learning
  numCitedBy: 64
  pid: 726df4f3f7a2ff193e626b49e37e2b3a7e494034
  show_ref_link: false
  title: ADC - Automated Deep Compression and Acceleration with Reinforcement Learning
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-apprentice-using-knowledge-distillation-techniques-to-improve-low-precision-network-accuracy
  numCitedBy: 236
  pid: cf8c44a703350ebc5df46a861c76db9f0e49457b
  show_ref_link: false
  title: Apprentice - Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-deep-residual-learning-for-image-recognition
  numCitedBy: 97653
  pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  show_ref_link: true
  title: Deep Residual Learning for Image Recognition
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift
  numCitedBy: 29648
  pid: 4d376d6978dad0374edfa6709c9556b42d3594d3
  show_ref_link: true
  title: Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-learning-transferable-architectures-for-scalable-image-recognition
  numCitedBy: 3538
  pid: d0611891b9e8a7c5731146097b6f201578f47b2f
  show_ref_link: true
  title: Learning Transferable Architectures for Scalable Image Recognition
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-identity-mappings-in-deep-residual-networks
  numCitedBy: 6555
  pid: 77f0a39b8e02686fd85b01971f8feb7f60971f80
  show_ref_link: true
  title: Identity Mappings in Deep Residual Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications
  numCitedBy: 10323
  pid: 3647d6d0f151dc05626449ee09cc7bce55be497e
  show_ref_link: true
  title: MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-distilling-the-knowledge-in-a-neural-network
  numCitedBy: 8901
  pid: 0c908739fbff75f03469d13d4a1a07de3414ee19
  show_ref_link: true
  title: Distilling the Knowledge in a Neural Network
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2018-inverted-residuals-and-linear-bottlenecks-mobile-networks-for-classification-detection-and-segmentation
  numCitedBy: 609
  pid: 16b42f570873fc03d503090adb0a75a467c5f30c
  show_ref_link: true
  title: Inverted Residuals and Linear Bottlenecks - Mobile Networks for Classification, Detection and Segmentation
  year: 2018
slug: Quantizing-deep-convolutional-networks-for-A-Krishnamoorthi
title: Quantizing deep convolutional networks for efficient inference - A whitepaper
url: https://www.semanticscholar.org/paper/Quantizing-deep-convolutional-networks-for-A-Krishnamoorthi/3d8b62c060f8444907e7c975c6ae590373b51ed4?sort=total-citations
venue: ArXiv
year: 2018
