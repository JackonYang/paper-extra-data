authors:
- Chenzhuo Zhu
- Song Han
- Huizi Mao
- W. Dally
badges:
- id: OPEN_ACCESS
corpusId: 224893
fieldsOfStudy:
- Computer Science
meta_key: 2017-trained-ternary-quantization
numCitedBy: 801
numCiting: 19
paperAbstract: Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it's as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16x smaller than full-precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.
ref_count: 19
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: 2017-quantized-neural-networks-training-neural-networks-with-low-precision-weights-and-activations
  numCitedBy: 1279
  pid: d2e4147eecae6f914e9e1e9aece8fdd2eaed809f
  show_ref_link: true
  title: Quantized Neural Networks - Training Neural Networks with Low Precision Weights and Activations
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations
  numCitedBy: 2134
  pid: a5733ff08daff727af834345b9cfff1d0aa109ec
  show_ref_link: true
  title: BinaryConnect - Training Deep Neural Networks with binary weights during propagations
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding
  numCitedBy: 5732
  pid: 642d0f49b7826adcf986616f4af77e736229990f
  show_ref_link: true
  title: Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-xnor-net-imagenet-classification-using-binary-convolutional-neural-networks
  numCitedBy: 2591
  pid: b649a98ce77ece8cd7638bb74ab77d22d9be77e7
  show_ref_link: true
  title: XNOR-Net - ImageNet Classification Using Binary Convolutional Neural Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2012-imagenet-classification-with-deep-convolutional-neural-networks
  numCitedBy: 82046
  pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  show_ref_link: true
  title: ImageNet classification with deep convolutional neural networks
  year: 2012
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-dorefa-net-training-low-bitwidth-convolutional-neural-networks-with-low-bitwidth-gradients
  numCitedBy: 1360
  pid: 8b053389eb8c18c61b84d7e59a95cb7e13f205b7
  show_ref_link: true
  title: DoReFa-Net - Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-neural-networks-with-few-multiplications
  numCitedBy: 280
  pid: 67c191bcce6821f736798cb9b31472bcdd1e52a6
  show_ref_link: true
  title: Neural Networks with Few Multiplications
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-deep-residual-learning-for-image-recognition
  numCitedBy: 97653
  pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  show_ref_link: true
  title: Deep Residual Learning for Image Recognition
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift
  numCitedBy: 29648
  pid: 4d376d6978dad0374edfa6709c9556b42d3594d3
  show_ref_link: true
  title: Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-eie-efficient-inference-engine-on-compressed-deep-neural-network
  numCitedBy: 1816
  pid: 2e2b189f668cf2c06ebc44dc9b166648256cf457
  show_ref_link: true
  title: EIE - Efficient Inference Engine on Compressed Deep Neural Network
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2009-learning-multiple-layers-of-features-from-tiny-images
  numCitedBy: 17474
  pid: 5d90f06bb70a0a3dced62413346235c02b1aa086
  show_ref_link: true
  title: Learning Multiple Layers of Features from Tiny Images
  year: 2009
- fieldsOfStudy:
  - Computer Science
  meta_key: 2014-caffe-convolutional-architecture-for-fast-feature-embedding
  numCitedBy: 13814
  pid: 6bdb186ec4726e00a8051119636d4df3b94043b5
  show_ref_link: true
  title: Caffe - Convolutional Architecture for Fast Feature Embedding
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-ternary-weight-networks
  numCitedBy: 721
  pid: 10a2482088e469dd40f49bdc9978b292b3f7bb1f
  show_ref_link: false
  title: Ternary Weight Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-adam-a-method-for-stochastic-optimization
  numCitedBy: 91740
  pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  show_ref_link: true
  title: Adam - A Method for Stochastic Optimization
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-identity-mappings-in-deep-residual-networks
  numCitedBy: 6555
  pid: 77f0a39b8e02686fd85b01971f8feb7f60971f80
  show_ref_link: true
  title: Identity Mappings in Deep Residual Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2016-deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin
  numCitedBy: 2250
  pid: 8ff840a40d3f1557c55c19d4d636da77103168ce
  show_ref_link: true
  title: Deep Speech 2 - End-to-End Speech Recognition in English and Mandarin
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: 2015-imagenet-large-scale-visual-recognition-challenge
  numCitedBy: 25826
  pid: e74f9b7f8eec6ba4704c206b93bc8079af3da4bd
  show_ref_link: true
  title: ImageNet Large Scale Visual Recognition Challenge
  year: 2015
slug: Trained-Ternary-Quantization-Zhu-Han
title: Trained Ternary Quantization
url: https://www.semanticscholar.org/paper/Trained-Ternary-Quantization-Zhu-Han/d418295cd3027c43eccc5592ae5b8303ba8192be?sort=total-citations
venue: ICLR
year: 2017
